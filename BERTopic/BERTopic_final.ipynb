{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install bertopic\n",
    "# %pip install indicnlp-library\n",
    "# %pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>news_id</th>\n",
       "      <th>news_date</th>\n",
       "      <th>news_category</th>\n",
       "      <th>news_title</th>\n",
       "      <th>news_article</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>6</td>\n",
       "      <td>1/6/2011 2:45:49 PM</td>\n",
       "      <td>மர்மம்</td>\n",
       "      <td>தூக்கில் தொங்கும் சேவல்கள் திருடர்களை காவு வாங...</td>\n",
       "      <td>நாலு ஆள் உயரம், முறுக்கு மீசை, கையில் வீச்சரிவ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>9</td>\n",
       "      <td>1/6/2011 2:56:51 PM</td>\n",
       "      <td>மர்மம்</td>\n",
       "      <td>பவுர்ணமி ஜாமத்தில் மாயமான கர்ப்பிணி</td>\n",
       "      <td>அமானுஷ்யமான சம்பவங்கள் நம்மை சுற்றி ஆங்காங்கே ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>11</td>\n",
       "      <td>1/6/2011 3:08:15 PM</td>\n",
       "      <td>மர்மம்</td>\n",
       "      <td>மச்சுபிச்சு மலை ரகசியம்</td>\n",
       "      <td>தென்அமெரிக்க நாடான பெருவில் காடுகள் மிகவும் பய...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>12</td>\n",
       "      <td>1/6/2011 3:09:20 PM</td>\n",
       "      <td>மர்மம்</td>\n",
       "      <td>ரத்த பலி வாங்கும் விபரீத ஆவி</td>\n",
       "      <td>கடந்த 18ம் தேதி சாயங்காலம்... அடைமழையை கிழித்த...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>13</td>\n",
       "      <td>1/6/2011 3:11:00 PM</td>\n",
       "      <td>மர்மம்</td>\n",
       "      <td>உலகப் பேரழகியின் மர்ம மரணம்</td>\n",
       "      <td>வரலாற்று பேரழகிகள் பட்டியலில் இன்றும் முதலிடத்...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49995</th>\n",
       "      <td>59792</td>\n",
       "      <td>7/16/2014 4:22:14 PM</td>\n",
       "      <td>உலகம்</td>\n",
       "      <td>லட்சம் கோடி முதலீட்டில் பிரிக்ஸ் வளர்ச்சி வங்க...</td>\n",
       "      <td>போர்டலிசா: பிரிக்ஸ் கூட்டமைப்பு நாடுகளின் வளர்...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49996</th>\n",
       "      <td>59793</td>\n",
       "      <td>7/16/2014 4:22:48 PM</td>\n",
       "      <td>தமிழகம்</td>\n",
       "      <td>ஆயிரம் விலைபஸ் ஸ்டாண்டில் கூவி கூவி குழந்தையை ...</td>\n",
       "      <td>திருமலை:பஸ் நிலையத்தில் பச்சிளம் பெண் குழந்தைக...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49997</th>\n",
       "      <td>59795</td>\n",
       "      <td>7/16/2014 4:23:57 PM</td>\n",
       "      <td>தமிழகம்</td>\n",
       "      <td>கோடி பறிமுதல் செய்யப்பட்ட வழக்கு கரகாட்டக்காரி...</td>\n",
       "      <td>வேலூர்:வேலூரை சேர்ந்த கரகாட்டக்காரி மோகனம்மாள்...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49998</th>\n",
       "      <td>59796</td>\n",
       "      <td>7/16/2014 4:59:42 PM</td>\n",
       "      <td>தமிழகம்</td>\n",
       "      <td>எழும்பூர் ரயில் நிலையத்தில் பிரீபெய்டு ஆட்டோ</td>\n",
       "      <td>சென்னை:சென்னை எழும்பூர் ரயில்நிலையத்தில் பிரீப...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49999</th>\n",
       "      <td>59797</td>\n",
       "      <td>7/17/2014 2:12:42 PM</td>\n",
       "      <td>இந்தியா</td>\n",
       "      <td>டெல்லியில் ஆட்சியமைக்க பாஜ தீவிர முயற்சி மோடி ...</td>\n",
       "      <td>புதுடெல்லி: கடந்த 6 மாதமாக ஜனாதிபதி ஆட்சியின் ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>50000 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       news_id             news_date news_category  \\\n",
       "0            6   1/6/2011 2:45:49 PM        மர்மம்   \n",
       "1            9   1/6/2011 2:56:51 PM        மர்மம்   \n",
       "2           11   1/6/2011 3:08:15 PM        மர்மம்   \n",
       "3           12   1/6/2011 3:09:20 PM        மர்மம்   \n",
       "4           13   1/6/2011 3:11:00 PM        மர்மம்   \n",
       "...        ...                   ...           ...   \n",
       "49995    59792  7/16/2014 4:22:14 PM         உலகம்   \n",
       "49996    59793  7/16/2014 4:22:48 PM       தமிழகம்   \n",
       "49997    59795  7/16/2014 4:23:57 PM       தமிழகம்   \n",
       "49998    59796  7/16/2014 4:59:42 PM       தமிழகம்   \n",
       "49999    59797  7/17/2014 2:12:42 PM       இந்தியா   \n",
       "\n",
       "                                              news_title  \\\n",
       "0      தூக்கில் தொங்கும் சேவல்கள் திருடர்களை காவு வாங...   \n",
       "1                    பவுர்ணமி ஜாமத்தில் மாயமான கர்ப்பிணி   \n",
       "2                                மச்சுபிச்சு மலை ரகசியம்   \n",
       "3                           ரத்த பலி வாங்கும் விபரீத ஆவி   \n",
       "4                            உலகப் பேரழகியின் மர்ம மரணம்   \n",
       "...                                                  ...   \n",
       "49995  லட்சம் கோடி முதலீட்டில் பிரிக்ஸ் வளர்ச்சி வங்க...   \n",
       "49996  ஆயிரம் விலைபஸ் ஸ்டாண்டில் கூவி கூவி குழந்தையை ...   \n",
       "49997  கோடி பறிமுதல் செய்யப்பட்ட வழக்கு கரகாட்டக்காரி...   \n",
       "49998       எழும்பூர் ரயில் நிலையத்தில் பிரீபெய்டு ஆட்டோ   \n",
       "49999  டெல்லியில் ஆட்சியமைக்க பாஜ தீவிர முயற்சி மோடி ...   \n",
       "\n",
       "                                            news_article  \n",
       "0      நாலு ஆள் உயரம், முறுக்கு மீசை, கையில் வீச்சரிவ...  \n",
       "1      அமானுஷ்யமான சம்பவங்கள் நம்மை சுற்றி ஆங்காங்கே ...  \n",
       "2      தென்அமெரிக்க நாடான பெருவில் காடுகள் மிகவும் பய...  \n",
       "3      கடந்த 18ம் தேதி சாயங்காலம்... அடைமழையை கிழித்த...  \n",
       "4      வரலாற்று பேரழகிகள் பட்டியலில் இன்றும் முதலிடத்...  \n",
       "...                                                  ...  \n",
       "49995  போர்டலிசா: பிரிக்ஸ் கூட்டமைப்பு நாடுகளின் வளர்...  \n",
       "49996  திருமலை:பஸ் நிலையத்தில் பச்சிளம் பெண் குழந்தைக...  \n",
       "49997  வேலூர்:வேலூரை சேர்ந்த கரகாட்டக்காரி மோகனம்மாள்...  \n",
       "49998  சென்னை:சென்னை எழும்பூர் ரயில்நிலையத்தில் பிரீப...  \n",
       "49999  புதுடெல்லி: கடந்த 6 மாதமாக ஜனாதிபதி ஆட்சியின் ...  \n",
       "\n",
       "[50000 rows x 5 columns]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get processed data\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "file_path = \"../data/\"\n",
    "\n",
    "data = pd.read_csv(file_path+\"train.csv\")\n",
    "\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to list\n",
    "df=data[\"news_title\"]\n",
    "\n",
    "docs = [str(i) for i in df.values]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import torch\n",
    "from transformers import BertModel, BertTokenizerFast\n",
    "\n",
    "tokenizer = BertTokenizerFast.from_pretrained(\"setu4993/LaBSE\")\n",
    "model = BertModel.from_pretrained(\"setu4993/LaBSE\")\n",
    "\n",
    "# Set model to evaluation mode\n",
    "model.eval()\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Encode a batch of documents using LaBSE model and return their embeddings.\n",
    "Args:\n",
    "    docs: List of strings representing the documents to be encoded.\n",
    "    batch_size: Size of the batch to be used during encoding.\n",
    "Returns:\n",
    "    embeddings: Tensor of shape (n_docs, embedding_size) representing the document embeddings.\n",
    "\"\"\"\n",
    "# Encode the documents in batches\n",
    "n_docs = len(docs)\n",
    "batch_size = 8\n",
    "embeds = torch.zeros((n_docs, model.config.hidden_size))\n",
    "for i in range(0, n_docs, batch_size):\n",
    "    batch = docs[i:i+batch_size]\n",
    "    inputs = tokenizer(batch, return_tensors=\"pt\", padding=True)\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "    batch_embeddings = outputs.pooler_output\n",
    "    embeds[i:i+batch_size] = batch_embeddings\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "file_path = \"../data/\"\n",
    "\n",
    "# save embeddings\n",
    "\n",
    "import pickle\n",
    "\n",
    "with open(file_path+'embeddings.pkl', 'wb') as f:\n",
    "\n",
    "    pickle.dump(embeds, f)\n",
    "\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fine tune UMAP - DIMENSIONALITY REDUCTION STEP\n",
    "\n",
    "from umap import UMAP\n",
    "\n",
    "\n",
    "umap_model = UMAP(n_neighbors=3, n_components=3, min_dist=0.05)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fine tune HDBSCAN - CLUSTERING STEP\n",
    "\n",
    "from hdbscan import HDBSCAN\n",
    "\n",
    "hdbscan_model = HDBSCAN(min_cluster_size=60, min_samples=30,\n",
    "                        prediction_data=True, gen_min_span_tree=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize docs trivially (split on spaces)\n",
    "\n",
    "from indicnlp.tokenize import sentence_tokenize, indic_tokenize\n",
    "\n",
    "def tokenize_ta(text,return_tensors=\"pt\",*args,**kwargs):\n",
    "    return indic_tokenize.trivial_tokenize(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Common list of stopwords\n",
    "\n",
    "stopwords=['அங்கு',\n",
    " 'அங்கே',\n",
    " 'அடுத்த',\n",
    " 'அதனால்',\n",
    " 'அதன்',\n",
    " 'அதற்கு',\n",
    " 'அதிக',\n",
    " 'அதில்',\n",
    " 'அது',\n",
    " 'அதே',\n",
    " 'அதை',\n",
    " 'அந்த',\n",
    " 'அந்தக்',\n",
    " 'அந்தப்',\n",
    " 'அன்று',\n",
    " 'அல்லது',\n",
    " 'அவன்',\n",
    " 'அவரது',\n",
    " 'அவர்',\n",
    " 'அவர்கள்',\n",
    " 'அவள்',\n",
    " 'அவை',\n",
    " 'ஆகிய',\n",
    " 'ஆகியோர்',\n",
    " 'ஆகும்',\n",
    " 'இங்கு',\n",
    " 'இங்கே',\n",
    " 'இடத்தில்',\n",
    " 'இடம்',\n",
    " 'இதனால்',\n",
    " 'இதனை',\n",
    " 'இதன்',\n",
    " 'இதற்கு',\n",
    " 'இதில்',\n",
    " 'இது',\n",
    " 'இதை',\n",
    " 'இந்த',\n",
    " 'இந்தக்',\n",
    " 'இந்தத்',\n",
    " 'இந்தப்',\n",
    " 'இன்னும்',\n",
    " 'இப்போது',\n",
    " 'இரு',\n",
    " 'இருக்கும்',\n",
    " 'இருந்த',\n",
    " 'இருந்தது',\n",
    " 'இருந்து',\n",
    " 'இவர்',\n",
    " 'இவை',\n",
    " 'உன்',\n",
    " 'உள்ள',\n",
    " 'உள்ளது',\n",
    " 'உள்ளன',\n",
    " 'எந்த',\n",
    " 'என',\n",
    " 'எனக்',\n",
    " 'எனக்கு',\n",
    " 'எனப்படும்',\n",
    " 'எனவும்',\n",
    " 'எனவே',\n",
    " 'எனினும்',\n",
    " 'எனும்',\n",
    " 'என்',\n",
    " 'என்ன',\n",
    " 'என்னும்',\n",
    " 'என்பது',\n",
    " 'என்பதை',\n",
    " 'என்ற',\n",
    " 'என்று',\n",
    " 'என்றும்',\n",
    " 'எல்லாம்',\n",
    " 'ஏன்',\n",
    " 'ஒரு',\n",
    " 'ஒரே',\n",
    " 'ஓர்',\n",
    " 'கொண்ட',\n",
    " 'கொண்டு',\n",
    " 'கொள்ள',\n",
    " 'சற்று',\n",
    " 'சிறு',\n",
    " 'சில',\n",
    " 'சேர்ந்த',\n",
    " 'தனது',\n",
    " 'தன்',\n",
    " 'தவிர',\n",
    " 'தான்',\n",
    " 'நான்',\n",
    " 'நாம்',\n",
    " 'நீ',\n",
    " 'பற்றி',\n",
    " 'பற்றிய',\n",
    " 'பல',\n",
    " 'பலரும்',\n",
    " 'பல்வேறு',\n",
    " 'பின்',\n",
    " 'பின்னர்',\n",
    " 'பிற',\n",
    " 'பிறகு',\n",
    " 'பெரும்',\n",
    " 'பேர்',\n",
    " 'போது',\n",
    " 'போன்ற',\n",
    " 'போல',\n",
    " 'போல்',\n",
    " 'மட்டுமே',\n",
    " 'மட்டும்',\n",
    " 'மற்ற',\n",
    " 'மற்றும்',\n",
    " 'மிக',\n",
    " 'மிகவும்',\n",
    " 'மீது',\n",
    " 'முதல்',\n",
    " 'முறை',\n",
    " 'மேலும்',\n",
    " 'மேல்',\n",
    " 'யார்',\n",
    " 'வந்த',\n",
    " 'வந்து',\n",
    " 'வரும்',\n",
    " 'வரை',\n",
    " 'வரையில்',\n",
    " 'விட',\n",
    " 'விட்டு',\n",
    " 'வேண்டும்',\n",
    " 'வேறு']\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# Create a vectorizer object to generate term document counts for topic representation - TOKENIZATION STEP\n",
    "\n",
    "vectorizer_model = CountVectorizer(\n",
    "    stop_words=stopwords,analyzer='word',\n",
    "    tokenizer=tokenize_ta\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from scipy.sparse import csr_matrix\n",
    "from typing import List, Mapping, Tuple, Union\n",
    "import stanza\n",
    "\n",
    "class TamilPOS():\n",
    "    \"\"\"\n",
    "    Extract Topic Keywords based on their Part-of-Speech using stanza library for Tamil.\n",
    "    \"\"\"\n",
    "    def __init__(self,\n",
    "                 top_n_words: int = 10,\n",
    "                 pos_patterns: List[str] = None):\n",
    "        self.top_n_words = top_n_words\n",
    "\n",
    "        if pos_patterns is None:\n",
    "            self.pos_patterns = [\n",
    "                'NOUN',\n",
    "                'PROPN',\n",
    "                'ADJ',\n",
    "            ]\n",
    "        else:\n",
    "            self.pos_patterns = pos_patterns\n",
    "\n",
    "        # load stanza pipeline for Tamil\n",
    "\n",
    "        self.nlp = stanza.Pipeline(lang='ta', processors='tokenize,pos')\n",
    "\n",
    "    def extract_topics(self,\n",
    "                       topic_model,\n",
    "                       documents: pd.DataFrame,\n",
    "                       c_tf_idf: csr_matrix,\n",
    "                       topics: Mapping[str, List[Tuple[str, float]]]\n",
    "                       ) -> Mapping[str, List[Tuple[str, float]]]:\n",
    "        topic_to_keywords = {}\n",
    "        for topic_id, topic_words in topics.items():\n",
    "\n",
    "            # filter candidate documents that contain at least one keyword from the topic\n",
    "\n",
    "            mask = documents['text'].str.contains('|'.join([word[0] for word in topic_words]), regex=True)\n",
    "            candidate_docs = documents[mask]\n",
    "\n",
    "            # extract candidate keywords from candidate_docs based on POS patterns\n",
    "\n",
    "            candidate_keywords = []\n",
    "            for doc in candidate_docs['text']:\n",
    "                doc_keywords = []\n",
    "\n",
    "                # get POS tags for each word in the document\n",
    "\n",
    "                doc_words = self.nlp(doc).sentences[0].words\n",
    "                for word in doc_words:\n",
    "                    if word.upos in self.pos_patterns:\n",
    "                        doc_keywords.append(word.text)\n",
    "                candidate_keywords.extend(doc_keywords)\n",
    "\n",
    "            # count the frequency of each keyword and keep the top n\n",
    "\n",
    "            candidate_keyword_counts = pd.Series(candidate_keywords).value_counts().head(self.top_n_words)\n",
    "\n",
    "            # normalize keyword counts\n",
    "\n",
    "            candidate_keyword_counts = candidate_keyword_counts / candidate_docs.shape[0]\n",
    "\n",
    "            # assign c-TF-IDF scores to keywords\n",
    "\n",
    "            keyword_scores = [(word, topic_model.get_topic(topic_id)[word]) for word in candidate_keyword_counts.index]\n",
    "\n",
    "            # sort keywords by their respective c-TF-IDF scores\n",
    "\n",
    "            sorted_keyword_scores = sorted(keyword_scores, key=lambda x: x[1], reverse=True)\n",
    "\n",
    "            # add top n keywords to topic_to_keywords dict\n",
    "            \n",
    "            topic_to_keywords[topic_id] = sorted_keyword_scores[:self.top_n_words]\n",
    "        return topic_to_keywords\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-04-30 21:18:11 INFO: Checking for updates to resources.json in case models have been updated.  Note: this behavior can be turned off with download_method=None or download_method=DownloadMethod.REUSE_RESOURCES\n",
      "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.5.0.json: 200kB [00:00, 17.1MB/s]                    \n",
      "2023-04-30 21:18:11 WARNING: Language ta package default expects mwt, which has been added\n",
      "2023-04-30 21:18:12 INFO: Loading these models for language: ta (Tamil):\n",
      "=======================\n",
      "| Processor | Package |\n",
      "-----------------------\n",
      "| tokenize  | ttb     |\n",
      "| mwt       | ttb     |\n",
      "| pos       | ttb     |\n",
      "=======================\n",
      "\n",
      "2023-04-30 21:18:12 INFO: Using device: cpu\n",
      "2023-04-30 21:18:12 INFO: Loading: tokenize\n",
      "2023-04-30 21:18:12 INFO: Loading: mwt\n",
      "2023-04-30 21:18:12 INFO: Loading: pos\n",
      "2023-04-30 21:18:12 INFO: Done loading processors!\n"
     ]
    }
   ],
   "source": [
    "representation_model = TamilPOS()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Create a BERTopic model\n",
    "\n",
    "from bertopic import BERTopic\n",
    "\n",
    "topic_model = BERTopic(\n",
    "    verbose=True,\n",
    "    calculate_probabilities=False,\n",
    "    embedding_model=model,\n",
    "    vectorizer_model=vectorizer_model,\n",
    "    umap_model=umap_model,\n",
    "    hdbscan_model=hdbscan_model,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-04-30 21:18:20,273 - BERTopic - Reduced dimensionality\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-04-30 21:18:22,237 - BERTopic - Clustered reduced embeddings\n"
     ]
    }
   ],
   "source": [
    "# Fit the model on the documents\n",
    "\n",
    "embeds_np = embeds.detach().numpy()\n",
    "topics = topic_model.fit_transform(docs,embeds_np)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Topic</th>\n",
       "      <th>Count</th>\n",
       "      <th>Name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-1</td>\n",
       "      <td>23676</td>\n",
       "      <td>-1_கைது_கொலை_பெண்_அரசு</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>912</td>\n",
       "      <td>0_ஆஸி_சதம்_வாட்சன்_டிராவிட்</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>780</td>\n",
       "      <td>1_ஒபாமா_பாகிஸ்தான்_அமெரிக்க_பாகிஸ்தானில்</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2</td>\n",
       "      <td>707</td>\n",
       "      <td>2_மின்_மக்கள்_குடிநீர்_மறியல்</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3</td>\n",
       "      <td>703</td>\n",
       "      <td>3_மோதி_லாரி_பலி_பைக்</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>138</th>\n",
       "      <td>137</td>\n",
       "      <td>63</td>\n",
       "      <td>137_மகன்_தந்தை_கொன்ற_கைது</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>139</th>\n",
       "      <td>138</td>\n",
       "      <td>62</td>\n",
       "      <td>138_பெட்ரோல்_வீச்சு_குண்டு_நிர்வாகி</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>140</th>\n",
       "      <td>139</td>\n",
       "      <td>61</td>\n",
       "      <td>139_மாவட்டத்தில்_திருவள்ளூர்_லிட்டர்_தண்ணீர்</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>141</th>\n",
       "      <td>140</td>\n",
       "      <td>61</td>\n",
       "      <td>140_நோயாளிகள்_மருத்துவமனையில்_ஆஸ்பத்திரியில்_அரசு</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>142</th>\n",
       "      <td>141</td>\n",
       "      <td>60</td>\n",
       "      <td>141_குமரியில்_குளிக்க_வெள்ளப்பெருக்கு_படகு</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>143 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Topic  Count                                               Name\n",
       "0       -1  23676                             -1_கைது_கொலை_பெண்_அரசு\n",
       "1        0    912                        0_ஆஸி_சதம்_வாட்சன்_டிராவிட்\n",
       "2        1    780           1_ஒபாமா_பாகிஸ்தான்_அமெரிக்க_பாகிஸ்தானில்\n",
       "3        2    707                      2_மின்_மக்கள்_குடிநீர்_மறியல்\n",
       "4        3    703                               3_மோதி_லாரி_பலி_பைக்\n",
       "..     ...    ...                                                ...\n",
       "138    137     63                          137_மகன்_தந்தை_கொன்ற_கைது\n",
       "139    138     62                138_பெட்ரோல்_வீச்சு_குண்டு_நிர்வாகி\n",
       "140    139     61       139_மாவட்டத்தில்_திருவள்ளூர்_லிட்டர்_தண்ணீர்\n",
       "141    140     61  140_நோயாளிகள்_மருத்துவமனையில்_ஆஸ்பத்திரியில்_அரசு\n",
       "142    141     60         141_குமரியில்_குளிக்க_வெள்ளப்பெருக்கு_படகு\n",
       "\n",
       "[143 rows x 3 columns]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get the topics\n",
    "\n",
    "topic_model.get_topic_info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-04-30 21:18:24 INFO: Checking for updates to resources.json in case models have been updated.  Note: this behavior can be turned off with download_method=None or download_method=DownloadMethod.REUSE_RESOURCES\n",
      "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.5.0.json: 200kB [00:00, 21.2MB/s]                    \n",
      "2023-04-30 21:18:24 WARNING: Language ta package default expects mwt, which has been added\n",
      "2023-04-30 21:18:25 INFO: Loading these models for language: ta (Tamil):\n",
      "=======================\n",
      "| Processor | Package |\n",
      "-----------------------\n",
      "| tokenize  | ttb     |\n",
      "| mwt       | ttb     |\n",
      "| pos       | ttb     |\n",
      "=======================\n",
      "\n",
      "2023-04-30 21:18:25 INFO: Using device: cpu\n",
      "2023-04-30 21:18:25 INFO: Loading: tokenize\n",
      "2023-04-30 21:18:25 INFO: Loading: mwt\n",
      "2023-04-30 21:18:25 INFO: Loading: pos\n",
      "2023-04-30 21:18:25 INFO: Done loading processors!\n"
     ]
    }
   ],
   "source": [
    "representation_model=TamilPOS(pos_patterns=['NOUN'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_model.update_topics(docs,representation_model=representation_model,vectorizer_model=vectorizer_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Topic</th>\n",
       "      <th>Count</th>\n",
       "      <th>Name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-1</td>\n",
       "      <td>23676</td>\n",
       "      <td>-1_கைது_கொலை_பெண்_அரசு</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>912</td>\n",
       "      <td>0_ஆஸி_சதம்_வாட்சன்_டிராவிட்</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>780</td>\n",
       "      <td>1_ஒபாமா_பாகிஸ்தான்_அமெரிக்க_பாகிஸ்தானில்</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2</td>\n",
       "      <td>707</td>\n",
       "      <td>2_மின்_மக்கள்_குடிநீர்_மறியல்</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3</td>\n",
       "      <td>703</td>\n",
       "      <td>3_மோதி_லாரி_பலி_பைக்</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>138</th>\n",
       "      <td>137</td>\n",
       "      <td>63</td>\n",
       "      <td>137_மகன்_தந்தை_கொன்ற_கைது</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>139</th>\n",
       "      <td>138</td>\n",
       "      <td>62</td>\n",
       "      <td>138_பெட்ரோல்_வீச்சு_குண்டு_நிர்வாகி</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>140</th>\n",
       "      <td>139</td>\n",
       "      <td>61</td>\n",
       "      <td>139_மாவட்டத்தில்_திருவள்ளூர்_லிட்டர்_தண்ணீர்</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>141</th>\n",
       "      <td>140</td>\n",
       "      <td>61</td>\n",
       "      <td>140_நோயாளிகள்_மருத்துவமனையில்_ஆஸ்பத்திரியில்_அரசு</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>142</th>\n",
       "      <td>141</td>\n",
       "      <td>60</td>\n",
       "      <td>141_குமரியில்_குளிக்க_வெள்ளப்பெருக்கு_படகு</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>143 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Topic  Count                                               Name\n",
       "0       -1  23676                             -1_கைது_கொலை_பெண்_அரசு\n",
       "1        0    912                        0_ஆஸி_சதம்_வாட்சன்_டிராவிட்\n",
       "2        1    780           1_ஒபாமா_பாகிஸ்தான்_அமெரிக்க_பாகிஸ்தானில்\n",
       "3        2    707                      2_மின்_மக்கள்_குடிநீர்_மறியல்\n",
       "4        3    703                               3_மோதி_லாரி_பலி_பைக்\n",
       "..     ...    ...                                                ...\n",
       "138    137     63                          137_மகன்_தந்தை_கொன்ற_கைது\n",
       "139    138     62                138_பெட்ரோல்_வீச்சு_குண்டு_நிர்வாகி\n",
       "140    139     61       139_மாவட்டத்தில்_திருவள்ளூர்_லிட்டர்_தண்ணீர்\n",
       "141    140     61  140_நோயாளிகள்_மருத்துவமனையில்_ஆஸ்பத்திரியில்_அரசு\n",
       "142    141     60         141_குமரியில்_குளிக்க_வெள்ளப்பெருக்கு_படகு\n",
       "\n",
       "[143 rows x 3 columns]"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "topic_model.get_topic_info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dill \n",
    "\n",
    "with open(\"../data/model.pkl\", \"wb\") as f:\n",
    "    model = dill.dump(topic_model, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
