{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#! pip install -q pyspark==3.3.0 spark-nlp==4.2.8\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries\n",
    "\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import sparknlp\n",
    "import pyspark.sql.functions as F\n",
    "\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.sql import SparkSession\n",
    "from sparknlp.annotator import *\n",
    "from sparknlp.base import *\n",
    "from sparknlp.pretrained import PretrainedPipeline\n",
    "from pyspark.sql.types import StringType, IntegerType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/04/27 20:20:04 WARN Utils: Your hostname, samyuktha.local resolves to a loopback address: 127.0.0.1; using 192.168.0.101 instead (on interface en0)\n",
      "23/04/27 20:20:04 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      ":: loading settings :: url = jar:file:/Users/samyuktha/opt/anaconda3/lib/python3.9/site-packages/pyspark/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ivy Default Cache set to: /Users/samyuktha/.ivy2/cache\n",
      "The jars for the packages stored in: /Users/samyuktha/.ivy2/jars\n",
      "com.johnsnowlabs.nlp#spark-nlp_2.12 added as a dependency\n",
      ":: resolving dependencies :: org.apache.spark#spark-submit-parent-8b85d76f-aae9-4ec9-8d14-3171370b3e05;1.0\n",
      "\tconfs: [default]\n",
      "\tfound com.johnsnowlabs.nlp#spark-nlp_2.12;4.2.8 in central\n",
      "\tfound com.typesafe#config;1.4.2 in central\n",
      "\tfound org.rocksdb#rocksdbjni;6.29.5 in central\n",
      "\tfound com.amazonaws#aws-java-sdk-bundle;1.11.828 in central\n",
      "\tfound com.github.universal-automata#liblevenshtein;3.0.0 in central\n",
      "\tfound com.google.protobuf#protobuf-java-util;3.0.0-beta-3 in central\n",
      "\tfound com.google.protobuf#protobuf-java;3.0.0-beta-3 in central\n",
      "\tfound com.google.code.gson#gson;2.3 in central\n",
      "\tfound it.unimi.dsi#fastutil;7.0.12 in central\n",
      "\tfound org.projectlombok#lombok;1.16.8 in central\n",
      "\tfound com.google.cloud#google-cloud-storage;2.15.0 in central\n",
      "\tfound com.google.guava#guava;31.1-jre in central\n",
      "\tfound com.google.guava#failureaccess;1.0.1 in central\n",
      "\tfound com.google.guava#listenablefuture;9999.0-empty-to-avoid-conflict-with-guava in central\n",
      "\tfound com.google.j2objc#j2objc-annotations;1.3 in central\n",
      "\tfound com.google.http-client#google-http-client;1.42.3 in central\n",
      "\tfound io.opencensus#opencensus-contrib-http-util;0.31.1 in central\n",
      "\tfound com.google.http-client#google-http-client-jackson2;1.42.3 in central\n",
      "\tfound com.google.http-client#google-http-client-gson;1.42.3 in central\n",
      "\tfound com.google.api-client#google-api-client;2.0.1 in central\n",
      "\tfound com.google.oauth-client#google-oauth-client;1.34.1 in central\n",
      "\tfound com.google.http-client#google-http-client-apache-v2;1.42.3 in central\n",
      "\tfound com.google.apis#google-api-services-storage;v1-rev20220705-2.0.0 in central\n",
      "\tfound com.google.code.gson#gson;2.10 in central\n",
      "\tfound com.google.cloud#google-cloud-core;2.8.27 in central\n",
      "\tfound com.google.auto.value#auto-value-annotations;1.10 in central\n",
      "\tfound com.google.cloud#google-cloud-core-http;2.8.27 in central\n",
      "\tfound com.google.http-client#google-http-client-appengine;1.42.3 in central\n",
      "\tfound com.google.api#gax-httpjson;0.104.5 in central\n",
      "\tfound com.google.cloud#google-cloud-core-grpc;2.8.27 in central\n",
      "\tfound io.grpc#grpc-core;1.50.2 in central\n",
      "\tfound com.google.api#gax;2.19.5 in central\n",
      "\tfound com.google.api#gax-grpc;2.19.5 in central\n",
      "\tfound com.google.auth#google-auth-library-credentials;1.12.1 in central\n",
      "\tfound com.google.auth#google-auth-library-oauth2-http;1.12.1 in central\n",
      "\tfound com.google.api#api-common;2.2.2 in central\n",
      "\tfound javax.annotation#javax.annotation-api;1.3.2 in central\n",
      "\tfound io.opencensus#opencensus-api;0.31.1 in central\n",
      "\tfound io.grpc#grpc-context;1.50.2 in central\n",
      "\tfound com.google.api.grpc#proto-google-iam-v1;1.6.7 in central\n",
      "\tfound com.google.protobuf#protobuf-java;3.21.9 in central\n",
      "\tfound com.google.protobuf#protobuf-java-util;3.21.9 in central\n",
      "\tfound com.google.api.grpc#proto-google-common-protos;2.10.0 in central\n",
      "\tfound org.threeten#threetenbp;1.6.4 in central\n",
      "\tfound com.google.api.grpc#proto-google-cloud-storage-v2;2.15.0-alpha in central\n",
      "\tfound com.google.api.grpc#grpc-google-cloud-storage-v2;2.15.0-alpha in central\n",
      "\tfound io.grpc#grpc-protobuf;1.50.2 in central\n",
      "\tfound com.google.api.grpc#gapic-google-cloud-storage-v2;2.15.0-alpha in central\n",
      "\tfound com.fasterxml.jackson.core#jackson-core;2.14.0 in central\n",
      "\tfound com.google.code.findbugs#jsr305;3.0.2 in central\n",
      "\tfound io.grpc#grpc-api;1.50.2 in central\n",
      "\tfound io.grpc#grpc-auth;1.50.2 in central\n",
      "\tfound io.grpc#grpc-stub;1.50.2 in central\n",
      "\tfound org.checkerframework#checker-qual;3.27.0 in central\n",
      "\tfound com.google.api.grpc#grpc-google-iam-v1;1.6.7 in central\n",
      "\tfound io.grpc#grpc-protobuf-lite;1.50.2 in central\n",
      "\tfound com.google.android#annotations;4.1.1.4 in central\n",
      "\tfound org.codehaus.mojo#animal-sniffer-annotations;1.22 in central\n",
      "\tfound io.grpc#grpc-alts;1.50.2 in central\n",
      "\tfound io.grpc#grpc-grpclb;1.50.2 in central\n",
      "\tfound org.conscrypt#conscrypt-openjdk-uber;2.5.2 in central\n",
      "\tfound io.grpc#grpc-netty-shaded;1.50.2 in central\n",
      "\tfound io.perfmark#perfmark-api;0.25.0 in central\n",
      "\tfound io.grpc#grpc-googleapis;1.50.2 in central\n",
      "\tfound io.grpc#grpc-xds;1.50.2 in central\n",
      "\tfound io.opencensus#opencensus-proto;0.2.0 in central\n",
      "\tfound io.grpc#grpc-services;1.50.2 in central\n",
      "\tfound com.google.re2j#re2j;1.6 in central\n",
      "\tfound com.navigamez#greex;1.0 in central\n",
      "\tfound dk.brics.automaton#automaton;1.11-8 in central\n",
      "\tfound com.johnsnowlabs.nlp#tensorflow-cpu_2.12;0.4.4 in central\n",
      ":: resolution report :: resolve 593ms :: artifacts dl 32ms\n",
      "\t:: modules in use:\n",
      "\tcom.amazonaws#aws-java-sdk-bundle;1.11.828 from central in [default]\n",
      "\tcom.fasterxml.jackson.core#jackson-core;2.14.0 from central in [default]\n",
      "\tcom.github.universal-automata#liblevenshtein;3.0.0 from central in [default]\n",
      "\tcom.google.android#annotations;4.1.1.4 from central in [default]\n",
      "\tcom.google.api#api-common;2.2.2 from central in [default]\n",
      "\tcom.google.api#gax;2.19.5 from central in [default]\n",
      "\tcom.google.api#gax-grpc;2.19.5 from central in [default]\n",
      "\tcom.google.api#gax-httpjson;0.104.5 from central in [default]\n",
      "\tcom.google.api-client#google-api-client;2.0.1 from central in [default]\n",
      "\tcom.google.api.grpc#gapic-google-cloud-storage-v2;2.15.0-alpha from central in [default]\n",
      "\tcom.google.api.grpc#grpc-google-cloud-storage-v2;2.15.0-alpha from central in [default]\n",
      "\tcom.google.api.grpc#grpc-google-iam-v1;1.6.7 from central in [default]\n",
      "\tcom.google.api.grpc#proto-google-cloud-storage-v2;2.15.0-alpha from central in [default]\n",
      "\tcom.google.api.grpc#proto-google-common-protos;2.10.0 from central in [default]\n",
      "\tcom.google.api.grpc#proto-google-iam-v1;1.6.7 from central in [default]\n",
      "\tcom.google.apis#google-api-services-storage;v1-rev20220705-2.0.0 from central in [default]\n",
      "\tcom.google.auth#google-auth-library-credentials;1.12.1 from central in [default]\n",
      "\tcom.google.auth#google-auth-library-oauth2-http;1.12.1 from central in [default]\n",
      "\tcom.google.auto.value#auto-value-annotations;1.10 from central in [default]\n",
      "\tcom.google.cloud#google-cloud-core;2.8.27 from central in [default]\n",
      "\tcom.google.cloud#google-cloud-core-grpc;2.8.27 from central in [default]\n",
      "\tcom.google.cloud#google-cloud-core-http;2.8.27 from central in [default]\n",
      "\tcom.google.cloud#google-cloud-storage;2.15.0 from central in [default]\n",
      "\tcom.google.code.findbugs#jsr305;3.0.2 from central in [default]\n",
      "\tcom.google.code.gson#gson;2.10 from central in [default]\n",
      "\tcom.google.guava#failureaccess;1.0.1 from central in [default]\n",
      "\tcom.google.guava#guava;31.1-jre from central in [default]\n",
      "\tcom.google.guava#listenablefuture;9999.0-empty-to-avoid-conflict-with-guava from central in [default]\n",
      "\tcom.google.http-client#google-http-client;1.42.3 from central in [default]\n",
      "\tcom.google.http-client#google-http-client-apache-v2;1.42.3 from central in [default]\n",
      "\tcom.google.http-client#google-http-client-appengine;1.42.3 from central in [default]\n",
      "\tcom.google.http-client#google-http-client-gson;1.42.3 from central in [default]\n",
      "\tcom.google.http-client#google-http-client-jackson2;1.42.3 from central in [default]\n",
      "\tcom.google.j2objc#j2objc-annotations;1.3 from central in [default]\n",
      "\tcom.google.oauth-client#google-oauth-client;1.34.1 from central in [default]\n",
      "\tcom.google.protobuf#protobuf-java;3.21.9 from central in [default]\n",
      "\tcom.google.protobuf#protobuf-java-util;3.21.9 from central in [default]\n",
      "\tcom.google.re2j#re2j;1.6 from central in [default]\n",
      "\tcom.johnsnowlabs.nlp#spark-nlp_2.12;4.2.8 from central in [default]\n",
      "\tcom.johnsnowlabs.nlp#tensorflow-cpu_2.12;0.4.4 from central in [default]\n",
      "\tcom.navigamez#greex;1.0 from central in [default]\n",
      "\tcom.typesafe#config;1.4.2 from central in [default]\n",
      "\tdk.brics.automaton#automaton;1.11-8 from central in [default]\n",
      "\tio.grpc#grpc-alts;1.50.2 from central in [default]\n",
      "\tio.grpc#grpc-api;1.50.2 from central in [default]\n",
      "\tio.grpc#grpc-auth;1.50.2 from central in [default]\n",
      "\tio.grpc#grpc-context;1.50.2 from central in [default]\n",
      "\tio.grpc#grpc-core;1.50.2 from central in [default]\n",
      "\tio.grpc#grpc-googleapis;1.50.2 from central in [default]\n",
      "\tio.grpc#grpc-grpclb;1.50.2 from central in [default]\n",
      "\tio.grpc#grpc-netty-shaded;1.50.2 from central in [default]\n",
      "\tio.grpc#grpc-protobuf;1.50.2 from central in [default]\n",
      "\tio.grpc#grpc-protobuf-lite;1.50.2 from central in [default]\n",
      "\tio.grpc#grpc-services;1.50.2 from central in [default]\n",
      "\tio.grpc#grpc-stub;1.50.2 from central in [default]\n",
      "\tio.grpc#grpc-xds;1.50.2 from central in [default]\n",
      "\tio.opencensus#opencensus-api;0.31.1 from central in [default]\n",
      "\tio.opencensus#opencensus-contrib-http-util;0.31.1 from central in [default]\n",
      "\tio.opencensus#opencensus-proto;0.2.0 from central in [default]\n",
      "\tio.perfmark#perfmark-api;0.25.0 from central in [default]\n",
      "\tit.unimi.dsi#fastutil;7.0.12 from central in [default]\n",
      "\tjavax.annotation#javax.annotation-api;1.3.2 from central in [default]\n",
      "\torg.checkerframework#checker-qual;3.27.0 from central in [default]\n",
      "\torg.codehaus.mojo#animal-sniffer-annotations;1.22 from central in [default]\n",
      "\torg.conscrypt#conscrypt-openjdk-uber;2.5.2 from central in [default]\n",
      "\torg.projectlombok#lombok;1.16.8 from central in [default]\n",
      "\torg.rocksdb#rocksdbjni;6.29.5 from central in [default]\n",
      "\torg.threeten#threetenbp;1.6.4 from central in [default]\n",
      "\t:: evicted modules:\n",
      "\tcom.google.protobuf#protobuf-java-util;3.0.0-beta-3 by [com.google.protobuf#protobuf-java-util;3.21.9] in [default]\n",
      "\tcom.google.protobuf#protobuf-java;3.0.0-beta-3 by [com.google.protobuf#protobuf-java;3.21.9] in [default]\n",
      "\tcom.google.code.gson#gson;2.3 by [com.google.code.gson#gson;2.10] in [default]\n",
      "\t---------------------------------------------------------------------\n",
      "\t|                  |            modules            ||   artifacts   |\n",
      "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
      "\t---------------------------------------------------------------------\n",
      "\t|      default     |   71  |   0   |   0   |   3   ||   68  |   0   |\n",
      "\t---------------------------------------------------------------------\n",
      ":: retrieving :: org.apache.spark#spark-submit-parent-8b85d76f-aae9-4ec9-8d14-3171370b3e05\n",
      "\tconfs: [default]\n",
      "\t0 artifacts copied, 68 already retrieved (0kB/17ms)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/04/27 20:20:05 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spark NLP Version : 4.2.8\n"
     ]
    }
   ],
   "source": [
    "# Create Spark Session\n",
    "\n",
    "spark = sparknlp.start()\n",
    "print (\"Spark NLP Version :\", sparknlp.version())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50000"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Load the dataset\n",
    "\n",
    "file_path = \"../data/\"\n",
    "\n",
    "file_type = \"csv\"\n",
    "\n",
    "\n",
    "# CSV options\n",
    "\n",
    "infer_schema = \"true\"\n",
    "first_row_is_header = \"true\"\n",
    "delimiter = \",\"\n",
    "multiLine=True\n",
    "escape='\"'\n",
    "\n",
    "# Read the file\n",
    "\n",
    "df = spark.read.format(file_type) \\\n",
    "  .option(\"inferSchema\", infer_schema) \\\n",
    "  .option(\"header\", first_row_is_header) \\\n",
    "  .option(\"sep\", delimiter) \\\n",
    "    .option(\"multiLine\", multiLine) \\\n",
    "    .option(\"escape\", escape) \\\n",
    "  .load(file_path+\"train.csv\")\n",
    "\n",
    "# Verify the count\n",
    "\n",
    "df.count()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lemma_ttb download started this may take some time.\n",
      "Approximate size to download 96.5 KB\n",
      "[ / ]lemma_ttb download started this may take some time.\n",
      "Approximate size to download 96.5 KB\n",
      "Download done! Loading the resource.\n",
      "[OK!]\n",
      "stopwords_iso download started this may take some time.\n",
      "Approximate size to download 1.9 KB\n",
      "[ | ]stopwords_iso download started this may take some time.\n",
      "Approximate size to download 1.9 KB\n",
      "Download done! Loading the resource.\n",
      "[OK!]\n"
     ]
    }
   ],
   "source": [
    "# Spark NLP requires the input dataframe or column to be converted to document.\n",
    " \n",
    "document_assembler = DocumentAssembler() \\\n",
    "    .setInputCol(\"news_title\") \\\n",
    "    .setOutputCol(\"document\") \\\n",
    "    .setCleanupMode(\"shrink\")\n",
    "\n",
    "# Split sentence to tokens(array)\n",
    "\n",
    "tokenizer = Tokenizer() \\\n",
    "  .setInputCols([\"document\"]) \\\n",
    "  .setOutputCol(\"token\")\n",
    "\n",
    "# Lemma is the process of converting a word to its base form. For example, the lemma of “was” is “be”, and the lemma of “rats” is “rat”.\n",
    "\n",
    "lemmatizer = LemmatizerModel.pretrained(\"lemma_ttb\", \"ta\").setInputCols([\"token\"]).setOutputCol(\"lemma\")\n",
    "\n",
    "# remove stop words\n",
    "\n",
    "stopwords_cleaner = StopWordsCleaner.pretrained(\"stopwords_iso\",\"ta\").setInputCols(\"lemma\").setOutputCol(\"cleanTokens\")\n",
    "\n",
    "\n",
    "# Finisher is the most important annotator. Spark NLP adds its own structure when we convert each row in the dataframe to document. Finisher helps us to bring back the expected structure viz. array of tokens.\n",
    "\n",
    "finisher = Finisher() \\\n",
    "    .setInputCols([\"cleanTokens\"]) \\\n",
    "    .setOutputCols([\"tokens\"]) \\\n",
    "    .setOutputAsArray(True) \\\n",
    "    .setCleanAnnotations(False)\n",
    "\n",
    "# We build a ml pipeline so that each phase can be executed in sequence. This pipeline can also be used to test the model. \n",
    "\n",
    "nlp_pipeline = Pipeline(\n",
    "    stages=[document_assembler, \n",
    "            tokenizer,\n",
    "            lemmatizer,\n",
    "            stopwords_cleaner,\n",
    "            finisher])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train the pipeline\n",
    "\n",
    "nlp_model = nlp_pipeline.fit(df)\n",
    "\n",
    "# apply the pipeline to transform dataframe.\n",
    "\n",
    "processed_df  = nlp_model.transform(df)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+--------------------+\n",
      "|          news_date|              tokens|\n",
      "+-------------------+--------------------+\n",
      "|1/6/2011 2:45:49 PM|[தூக்கில், தொங்கு...|\n",
      "|1/6/2011 2:56:51 PM|[பவுர்ணமி, ஜாமத்த...|\n",
      "|1/6/2011 3:08:15 PM|[மச்சுபிச்சு, மலை...|\n",
      "|1/6/2011 3:09:20 PM|[ரத்த, பலி, வாங்க...|\n",
      "|1/6/2011 3:11:00 PM|[உலகப், பேரழகியின...|\n",
      "|1/6/2011 3:17:03 PM|[அமாவாசை, இருட்டி...|\n",
      "|1/6/2011 3:17:57 PM|[நடுக்கடலில், மிர...|\n",
      "|1/6/2011 3:19:43 PM|             [பூதம்]|\n",
      "|1/6/2011 3:23:57 PM|[காவிரி, கரையில்,...|\n",
      "|1/6/2011 3:25:59 PM|[இருட்டில், துரத்...|\n",
      "|1/6/2011 3:26:40 PM|[கதிகலங்க, வைக்கு...|\n",
      "|1/6/2011 3:32:36 PM|[கல்லறையை, காவல்,...|\n",
      "|1/6/2011 3:40:26 PM|[காக்க, காக்க, கன...|\n",
      "|1/6/2011 3:41:21 PM|   [வைகுண்ட, ஏகாதசி]|\n",
      "|1/6/2011 3:42:08 PM|[கிரகங்கள், அருளு...|\n",
      "|1/6/2011 3:42:20 PM|[நியூசிபாக், டெஸ்...|\n",
      "|1/6/2011 3:42:58 PM|[துர்க்கா, தேவியை...|\n",
      "|1/6/2011 3:43:31 PM|[நவராத்திரி, சுபர...|\n",
      "|1/6/2011 3:44:14 PM|[ஆறு, பலங்கள், தர...|\n",
      "|1/6/2011 3:44:45 PM|[டெஸ்ட், போட்டிக்...|\n",
      "+-------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# nlp pipeline create intermediary columns that we dont need. We can drop them.\n",
    "\n",
    "tokens_df = processed_df.select('news_date','tokens')\n",
    "tokens_df.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the array of tokens to string\n",
    "\n",
    "df1=tokens_df.withColumn(\"tokens\", F.array_join(F.col(\"tokens\"), \",\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "\n",
    "# Save the processed data to csv\n",
    "\n",
    "df1.toPandas().to_csv(file_path+\"processed_data.csv\", index=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "7a2195d159a3a82bc7d00c6e10208c7379d234f070de3315b31db35494453c1f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
