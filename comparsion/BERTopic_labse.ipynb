{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get processed data\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "file_path = \"../data/\"\n",
    "\n",
    "data = pd.read_csv(file_path+\"train.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to list\n",
    "df=data[\"news_title\"]\n",
    "\n",
    "docs = [str(i) for i in df.values]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/samyuktha/opt/anaconda3/envs/ml/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import torch\n",
    "from transformers import BertModel, BertTokenizerFast\n",
    "\n",
    "tokenizer = BertTokenizerFast.from_pretrained(\"setu4993/LaBSE\")\n",
    "model = BertModel.from_pretrained(\"setu4993/LaBSE\")\n",
    "\n",
    "# Set model to evaluation mode\n",
    "model.eval()\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Encode a batch of documents using LaBSE model and return their embeddings.\n",
    "Args:\n",
    "    docs: List of strings representing the documents to be encoded.\n",
    "    batch_size: Size of the batch to be used during encoding.\n",
    "Returns:\n",
    "    embeddings: Tensor of shape (n_docs, embedding_size) representing the document embeddings.\n",
    "\"\"\"\n",
    "# Encode the documents in batches\n",
    "n_docs = len(docs)\n",
    "batch_size = 8\n",
    "embeds = torch.zeros((n_docs, model.config.hidden_size))\n",
    "for i in range(0, n_docs, batch_size):\n",
    "    batch = docs[i:i+batch_size]\n",
    "    inputs = tokenizer(batch, return_tensors=\"pt\", padding=True)\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "    batch_embeddings = outputs.pooler_output\n",
    "    embeds[i:i+batch_size] = batch_embeddings\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize docs trivially (split on spaces)\n",
    "\n",
    "from indicnlp.tokenize import sentence_tokenize, indic_tokenize\n",
    "\n",
    "def tokenize_ta(text,return_tensors=\"pt\",*args,**kwargs):\n",
    "    return indic_tokenize.trivial_tokenize(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Common list of stopwords\n",
    "\n",
    "stopwords=['அங்கு',\n",
    " 'அங்கே',\n",
    " 'அடுத்த',\n",
    " 'அதனால்',\n",
    " 'அதன்',\n",
    " 'அதற்கு',\n",
    " 'அதிக',\n",
    " 'அதில்',\n",
    " 'அது',\n",
    " 'அதே',\n",
    " 'அதை',\n",
    " 'அந்த',\n",
    " 'அந்தக்',\n",
    " 'அந்தப்',\n",
    " 'அன்று',\n",
    " 'அல்லது',\n",
    " 'அவன்',\n",
    " 'அவரது',\n",
    " 'அவர்',\n",
    " 'அவர்கள்',\n",
    " 'அவள்',\n",
    " 'அவை',\n",
    " 'ஆகிய',\n",
    " 'ஆகியோர்',\n",
    " 'ஆகும்',\n",
    " 'இங்கு',\n",
    " 'இங்கே',\n",
    " 'இடத்தில்',\n",
    " 'இடம்',\n",
    " 'இதனால்',\n",
    " 'இதனை',\n",
    " 'இதன்',\n",
    " 'இதற்கு',\n",
    " 'இதில்',\n",
    " 'இது',\n",
    " 'இதை',\n",
    " 'இந்த',\n",
    " 'இந்தக்',\n",
    " 'இந்தத்',\n",
    " 'இந்தப்',\n",
    " 'இன்னும்',\n",
    " 'இப்போது',\n",
    " 'இரு',\n",
    " 'இருக்கும்',\n",
    " 'இருந்த',\n",
    " 'இருந்தது',\n",
    " 'இருந்து',\n",
    " 'இவர்',\n",
    " 'இவை',\n",
    " 'உன்',\n",
    " 'உள்ள',\n",
    " 'உள்ளது',\n",
    " 'உள்ளன',\n",
    " 'எந்த',\n",
    " 'என',\n",
    " 'எனக்',\n",
    " 'எனக்கு',\n",
    " 'எனப்படும்',\n",
    " 'எனவும்',\n",
    " 'எனவே',\n",
    " 'எனினும்',\n",
    " 'எனும்',\n",
    " 'என்',\n",
    " 'என்ன',\n",
    " 'என்னும்',\n",
    " 'என்பது',\n",
    " 'என்பதை',\n",
    " 'என்ற',\n",
    " 'என்று',\n",
    " 'என்றும்',\n",
    " 'எல்லாம்',\n",
    " 'ஏன்',\n",
    " 'ஒரு',\n",
    " 'ஒரே',\n",
    " 'ஓர்',\n",
    " 'கொண்ட',\n",
    " 'கொண்டு',\n",
    " 'கொள்ள',\n",
    " 'சற்று',\n",
    " 'சிறு',\n",
    " 'சில',\n",
    " 'சேர்ந்த',\n",
    " 'தனது',\n",
    " 'தன்',\n",
    " 'தவிர',\n",
    " 'தான்',\n",
    " 'நான்',\n",
    " 'நாம்',\n",
    " 'நீ',\n",
    " 'பற்றி',\n",
    " 'பற்றிய',\n",
    " 'பல',\n",
    " 'பலரும்',\n",
    " 'பல்வேறு',\n",
    " 'பின்',\n",
    " 'பின்னர்',\n",
    " 'பிற',\n",
    " 'பிறகு',\n",
    " 'பெரும்',\n",
    " 'பேர்',\n",
    " 'போது',\n",
    " 'போன்ற',\n",
    " 'போல',\n",
    " 'போல்',\n",
    " 'மட்டுமே',\n",
    " 'மட்டும்',\n",
    " 'மற்ற',\n",
    " 'மற்றும்',\n",
    " 'மிக',\n",
    " 'மிகவும்',\n",
    " 'மீது',\n",
    " 'முதல்',\n",
    " 'முறை',\n",
    " 'மேலும்',\n",
    " 'மேல்',\n",
    " 'யார்',\n",
    " 'வந்த',\n",
    " 'வந்து',\n",
    " 'வரும்',\n",
    " 'வரை',\n",
    " 'வரையில்',\n",
    " 'விட',\n",
    " 'விட்டு',\n",
    " 'வேண்டும்',\n",
    " 'வேறு']\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# Create a vectorizer object to generate term document counts for topic representation - TOKENIZATION STEP\n",
    "\n",
    "vectorizer_model = CountVectorizer(\n",
    "    stop_words=stopwords,analyzer='word',\n",
    "    tokenizer=tokenize_ta\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Create a BERTopic model\n",
    "\n",
    "from bertopic import BERTopic\n",
    "\n",
    "topic_model = BERTopic(\n",
    "    vectorizer_model=vectorizer_model,\n",
    "    verbose=True,\n",
    "    calculate_probabilities=False,\n",
    "    embedding_model=model,\n",
    "    \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "OMP: Info #276: omp_set_nested routine deprecated, please use omp_set_max_active_levels instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-04-19 21:43:34,165 - BERTopic - Reduced dimensionality\n",
      "2023-04-19 21:43:34,380 - BERTopic - Clustered reduced embeddings\n"
     ]
    }
   ],
   "source": [
    "# Fit the model on the documents\n",
    "embeds_np = embeds.detach().numpy()\n",
    "\n",
    "topics = topic_model.fit_transform(docs,embeds_np)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim.corpora as corpora\n",
    "from gensim.models.coherencemodel import CoherenceModel\n",
    "\n",
    "# Preprocess documents\n",
    "\n",
    "cleaned_docs = topic_model._preprocess_text(docs)\n",
    "\n",
    "# Extract vectorizer and tokenizer from BERTopic\n",
    "\n",
    "vectorizer = topic_model.vectorizer_model\n",
    "tokenizer = vectorizer.build_tokenizer()\n",
    "\n",
    "# Extract features for Topic Coherence evaluation\n",
    "\n",
    "words = vectorizer.get_feature_names_out()\n",
    "tokens = [tokenizer(doc) for doc in cleaned_docs]\n",
    "dictionary = corpora.Dictionary(tokens)\n",
    "corpus = [dictionary.doc2bow(token) for token in tokens]\n",
    "topic_words = [[words for words, _ in topic_model.get_topic(topic)] \n",
    "               for topic in range(len(topic_model.get_topics())-1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Coherence Score: 0.57506335\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Evaluate\n",
    "\n",
    "from gensim.models import KeyedVectors\n",
    "from gensim.models import word2vec\n",
    "from gensim import corpora, models\n",
    "\n",
    "# Load pre-trained Word2Vec model for Tamil language\n",
    "model_path = \"/Users/samyuktha/FYP/data/cc.ta.300.vec.gz\"\n",
    "w2v_model = KeyedVectors.load_word2vec_format(model_path)\n",
    "\n",
    "# Compute coherence score using Word2Vec similarity measure\n",
    "\n",
    "cm = CoherenceModel(topics=topic_words, \n",
    "                                 texts=tokens, \n",
    "                                 corpus=corpus,\n",
    "                                 dictionary=dictionary, \n",
    "coherence='c_w2v',keyed_vectors=w2v_model)\n",
    "\n",
    "coherence_score = cm.get_coherence()\n",
    "\n",
    "print(\"Coherence Score:\", coherence_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/samyuktha/opt/anaconda3/envs/ml/lib/python3.9/site-packages/numpy/core/fromnumeric.py:3432: RuntimeWarning: Mean of empty slice.\n",
      "  return _methods._mean(a, axis=axis, dtype=dtype,\n",
      "/Users/samyuktha/opt/anaconda3/envs/ml/lib/python3.9/site-packages/numpy/core/_methods.py:190: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": [
    "coherences=['u_mass', 'c_v', 'c_uci', 'c_npmi']\n",
    "scores=[]\n",
    "for coherence in coherences:\n",
    "    cm = CoherenceModel(topics=topic_words, \n",
    "                                 texts=tokens, \n",
    "                                 corpus=corpus,\n",
    "                                 dictionary=dictionary,\n",
    "                                    coherence=coherence)\n",
    "    coherence_score = cm.get_coherence()\n",
    "    scores.append(coherence_score)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "u_mass nan\n",
      "c_v 0.36932059170424336\n",
      "c_uci nan\n",
      "c_npmi nan\n"
     ]
    }
   ],
   "source": [
    "#print scores\n",
    "\n",
    "for i in range(len(coherences)):\n",
    "    print(coherences[i],scores[i])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
